{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for loading images and extracting patches based on nuclear density\n",
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_path, transform=None, patch_size=299, overlap=0.5, threshold=1.587, min_blue_density=0.02):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.overlap = overlap\n",
    "        self.threshold = threshold\n",
    "        self.min_blue_density = min_blue_density\n",
    "\n",
    "        # Load CSV containing file names and labels\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.image_paths = [os.path.join(image_dir, fname) for fname in self.data.iloc[:, 0]]\n",
    "        self.labels = self.data.iloc[:, 1].values\n",
    "\n",
    "        # Create a label-to-index mapping if your labels are strings\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(np.unique(self.labels))}\n",
    "        self.labels = np.array([self.label_to_idx[label] for label in self.labels], dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load the image using PIL and convert to RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Extract patches from the image\n",
    "        patches, patch_labels = self.extract_patches(image, label)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            patches = [self.transform(patch) for patch in patches]\n",
    "\n",
    "        return patches, patch_labels\n",
    "\n",
    "    def extract_patches(self, image, label):\n",
    "        patches = []\n",
    "        patch_labels = []\n",
    "        height, width, _ = image.shape\n",
    "        stride = int(self.patch_size * (1 - self.overlap))\n",
    "\n",
    "        # Slide window to extract patches\n",
    "        for y in range(0, height - self.patch_size + 1, stride):\n",
    "            for x in range(0, width - self.patch_size + 1, stride):\n",
    "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                if self.is_nucleus_dense(patch):\n",
    "                    patches.append(patch)\n",
    "                    patch_labels.append(label)\n",
    "\n",
    "        return patches, patch_labels\n",
    "\n",
    "    def is_nucleus_dense(self, patch):\n",
    "        # Convert patch to HSV and extract the blue channel\n",
    "        hsv_patch = cv2.cvtColor(patch, cv2.COLOR_RGB2HSV)\n",
    "        blue_mask = (hsv_patch[:, :, 0] > self.threshold).astype(np.uint8)\n",
    "        blue_density = np.sum(blue_mask) / (patch.shape[0] * patch.shape[1])\n",
    "        return blue_density > self.min_blue_density\n",
    "\n",
    "# transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModifiedInceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ModifiedInceptionV3, self).__init__()\n",
    "        self.inception = models.inception_v3(pretrained=True)\n",
    "        self.inception.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inception.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.inception.fc.in_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The inception_v3 model returns two outputs (logits and auxiliary logits)\n",
    "        # only retain primary logits\n",
    "        outputs = self.inception(x)\n",
    "        if isinstance(outputs, tuple):\n",
    "            # Extract the primary output from the tuple (discard the auxiliary output)\n",
    "            outputs = outputs[0]\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for patches, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            patches = torch.cat([patch.to(device) for patch in patches], dim=0)  # Combine patches into a single batch\n",
    "            \n",
    "            # Flatten the nested list/tensor structure of labels\n",
    "            if isinstance(labels, list) or isinstance(labels, tuple):\n",
    "                labels = torch.cat(labels, dim=0).to(device)  # Flatten into a single tensor\n",
    "\n",
    "            #print(f\"Flattened Labels: {labels}\")  #  Print flattened labels\n",
    "\n",
    "            # Convert labels to the appropriate tensor type\n",
    "            labels = labels.long()  # Ensure it's long type for classification\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(patches)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"Train Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for patches, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                patches = torch.cat([patch.to(device) for patch in patches], dim=0)\n",
    "\n",
    "                # Flatten the nested list/tensor structure of labels\n",
    "                if isinstance(labels, list) or isinstance(labels, tuple):\n",
    "                    labels = torch.cat(labels, dim=0).to(device)  # Flatten into a single tensor\n",
    "\n",
    "                # Ensure labels are in the correct format\n",
    "                labels = labels.long()\n",
    "\n",
    "                outputs = model(patches)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# Majority Voting for Final Image Classification\n",
    "def majority_voting(patch_predictions):\n",
    "    # Perform majority voting\n",
    "    values, counts = np.unique(patch_predictions, return_counts=True)\n",
    "    majority_class = values[np.argmax(counts)]\n",
    "    return majority_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "image_dir = r\"C:\\Users\\vamsv\\Downloads\\ICIAR2018_BACH_Challenge\\ICIAR2018_BACH_Challenge\\Photos\\images\"  \n",
    "csv_path = r\"C:\\Users\\vamsv\\Downloads\\ICIAR2018_BACH_Challenge\\ICIAR2018_BACH_Challenge\\Photos\\microscopy_ground_truth.csv\"  \n",
    "\n",
    "dataset = HistologyDataset(image_dir=image_dir, csv_path=csv_path, transform=transform)\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ModifiedInceptionV3(num_classes=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
